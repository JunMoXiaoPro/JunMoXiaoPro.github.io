<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>君莫笑的博客</title>
  
  <subtitle>JunMoXiao</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://junmoxiaopro.github.io/"/>
  <updated>2019-09-30T06:33:38.672Z</updated>
  <id>https://junmoxiaopro.github.io/</id>
  
  <author>
    <name>JunMoXiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CDH安装</title>
    <link href="https://junmoxiaopro.github.io/CDH%E5%AE%89%E8%A3%85.html"/>
    <id>https://junmoxiaopro.github.io/CDH安装.html</id>
    <published>2019-09-30T06:33:08.746Z</published>
    <updated>2019-09-30T06:33:38.672Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>微信公众号：<strong><a href="#jump_20">关注菜鸟解说大数据</a></strong><br>关注可了解更多的大数据相关的内容。问题或建议，请公众号留言;<br><strong><a href="#jump_20">如果你觉得我写的文章对你有帮助，欢迎关注和赞赏我</a>[^1]</strong></p></blockquote><p>[TOC]</p><h3 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h3><ul><li>1.<a href="https://mp.weixin.qq.com/s/GOdvWHxx11pHdURx9G9hcA" target="_blank" rel="noopener">在腾讯云中安装mysql</a></li><li>2.<a href="https://mp.weixin.qq.com/s/4QHc02NnmL8rVnY0q4tohg" target="_blank" rel="noopener">shell脚本出来Mysql的增删改查</a></li><li>3.<a href="https://mp.weixin.qq.com/s/DB-d3o-Zr6ma_5LlgUfcjQ" target="_blank" rel="noopener">MYSQL中limit不适用的场景</a></li><li>4.<a href="https://mp.weixin.qq.com/s/NPzgIx-UogYBTuut-Fe5ZA" target="_blank" rel="noopener">用shell玩转MYSQL实战</a></li></ul><h4 id="1-CDH简介"><a href="#1-CDH简介" class="headerlink" title="1. CDH简介"></a>1. CDH简介</h4><p>Cloudera Manager 最主要的功能就是安装和管理CDH服务，可以使用Cloudera Manager 来为新环境安装服务也可以用来升级现有的服务，在安装方式上，Cloudera Manager支持两种格式：packages和parcels。</p><p> Package：是一种二进制的软件包，包括了编译后的代码和元信息，比如包的描述、版本和依赖，通过这些元信息，我们可以使用包管理器来搜索和安装需要的软件包，Cloudera Manager使用操作系统的包管理器来安装软件，如Redhat的rpm包管理和Ubuntu的apt-get等。</p><p> Parcel：是一种包含二进制程序文件的包，同时还包含额外的元数据信息，是Cloudera Manager使用的一种包管理和安装CDH服务的方式。它和package的不同之处主要有以下几点：</p><p> Parcels是一组包含特定版本组件的包，也可以下载多个不同的版本到同一个parcel目录下，在安装时可以选择要激活的组件版本。</p><p> 可以安装parcels到系统的任意位置，默认安装位置为：/opt/cloudera/parcels，package包的默认安装位置为：/usr/lib。</p><p> 当从Cloudera Manager的Parcels page安装时，Cloudera Manager会为集群中的每个节点自动下载、分发和激活选定的parcel。</p><h4 id="2-总的架构"><a href="#2-总的架构" class="headerlink" title="2. 总的架构"></a>2. 总的架构</h4><p><img src="https://ws1.sinaimg.cn/large/006ZE1GEgy1g3w3yvlexij30jn0esac6.jpg" alt="图片来源于网络"></p><h4 id="3-集群规划"><a href="#3-集群规划" class="headerlink" title="3. 集群规划"></a>3. 集群规划</h4><ul><li>HDFS NameNode (2, HA)</li><li>HDFS JournalNode (3)</li><li>HDFS FailoverController (2, collocated with NN services)</li><li>YARN Resource Manager (2, HA)</li><li>YARN History Server</li><li>HBase Master (3, HA)</li><li>Sentry Service</li><li>ZooKeeper (3)</li></ul><h4 id="4-集群部署"><a href="#4-集群部署" class="headerlink" title="4. 集群部署"></a>4. 集群部署</h4><h5 id="4-1-安装介质准备"><a href="#4-1-安装介质准备" class="headerlink" title="4.1. 安装介质准备"></a>4.1. 安装介质准备</h5><p>1.JDK1.8 下载地址：<br>链接：<a href="https://pan.baidu.com/s/1wiM5de1B8Tk67y1TyPQQUg" target="_blank" rel="noopener">https://pan.baidu.com/s/1wiM5de1B8Tk67y1TyPQQUg</a> 密码：skoe<br> 2.Cloudera Manager 下载地址：</p><p><a href="https://archive.cloudera.com/cm5/cm/5/" target="_blank" rel="noopener">https://archive.cloudera.com/cm5/cm/5/</a></p><p>说明：CM的各个平台，版本的包都在这里了，各取所需。根据我的环境，下载的是 cloudera-manager-centos7-cm5.15.2_x86_64.tar.gz</p><p>3.CDH 下载地址：</p><p><a href="http://archive.cloudera.com/cdh5/parcels/5.14/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/5.14/</a></p><p> 说明：和上面CM一样，各取所需。需要下载3个文件，我下载的是 CDH-5.15.2-1.cdh5.15.5.p0.3-el7.parcel.sha1（这个文件下载下来后，一定要把后缀 .sha1 改为 .sha）， CDH-5.15.2-1.cdh5.15.2.p0.3-el7.parcel， manifest.json</p><p>这里要注意CM和CDH，大版本应该要一致，比如CM为5.15，CDH也是5.15</p><p>4.Mysql<br>如果你的机器能连接外网，那么这里可以暂时不用准备，下面联网安装。如果不能连接外网，那么这里就可以先安装好MySQL（为了方便，安装在server上）。<br>5.数据库驱动 下载地址：</p><p>链接：<a href="https://pan.baidu.com/s/1yAS_LG88-uOInu5Ee4vzzQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1yAS_LG88-uOInu5Ee4vzzQ</a> 密码：1bf6</p><p>说明：注意默认的数据驱动是有版本号的，比如mysql-connector-java-xxx.jar，这里要把版本号去掉</p><p> 6.mysql-community-libs-compat-5.7.23-1.el7.x86_64.rpm 下载地址：</p><p>链接：<a href="https://pan.baidu.com/s/1PqHf3XCoAsDeEqwiDMH_5Q" target="_blank" rel="noopener">https://pan.baidu.com/s/1PqHf3XCoAsDeEqwiDMH_5Q</a> 密码：qgcq</p><p>说明：这是MySQL的一个组件的安装包，不是核心组件，不安装也不影响MySQL使用。但是如果缺少这个包，后面hue安装的时候很可能报错，所以先下载下来备用。如果想找不同的版本，可以去MySQL官网找。</p><h5 id="4-2-安装目录规划"><a href="#4-2-安装目录规划" class="headerlink" title="4.2 安装目录规划"></a>4.2 安装目录规划</h5><p>1.JDK安装目录：/usr/lib/jdk<br>2.CDH Manager安装目录：/opt/cloudera-manager<br>3.CDH 组件安装目录：/opt/cloudera</p><h5 id="4-3-配置主机名和hosts"><a href="#4-3-配置主机名和hosts" class="headerlink" title="4.3 配置主机名和hosts"></a>4.3 配置主机名和hosts</h5><p>1) 执行下面命令，配置静态IP等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure><p>下面这是我的网卡配置信息，仅供参考</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=4cbacedd-16ea-4204-9829-223958abbf62</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.79.110</span><br><span class="line">GATEWAY=192.168.79.2</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">DNS1=192.168.79.2</span><br></pre></td></tr></table></figure><p>2)修改hosts文件，输入IP和主机名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><p>下面是我的hosts文件，仅供参考：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.79.100    hadoop01</span><br><span class="line">192.168.79.110    hadoop02</span><br><span class="line">192.168.79.120    hadoop03</span><br></pre></td></tr></table></figure><h5 id="4-4-关闭防火墙"><a href="#4-4-关闭防火墙" class="headerlink" title="4.4 关闭防火墙"></a>4.4 关闭防火墙</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure><p>重启机器。然后保证主机之间互ping 主机名，互ping ip能够ping通</p><h5 id="4-5-配置免密码登陆（每台主机都要做）"><a href="#4-5-配置免密码登陆（每台主机都要做）" class="headerlink" title="4.5 配置免密码登陆（每台主机都要做）"></a>4.5 配置免密码登陆（每台主机都要做）</h5><p>由于时间的原因，后面的内容，我们下期再见。</p><h4 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h4><p><a href="https://blog.csdn.net/u010936936/article/details/81939880#%E5%BC%80%E5%A7%8B%E5%AE%89%E8%A3%85CM%E5%92%8CCDH" target="_blank" rel="noopener">https://blog.csdn.net/u010936936/article/details/81939880#%E5%BC%80%E5%A7%8B%E5%AE%89%E8%A3%85CM%E5%92%8CCDH</a><br><a href="https://www.cnblogs.com/xiqing/p/9645718.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiqing/p/9645718.html</a><br><a id="footnote-1"></a><br><a id="jump_20"></a></p><h3 id="关注菜鸟解说大数据"><a href="#关注菜鸟解说大数据" class="headerlink" title="关注菜鸟解说大数据"></a>关注菜鸟解说大数据</h3><p>如果你觉得到作者的文章对你有帮助，欢迎赞赏，有你的支持，公众号一定会越来越好！<br><img src="https://ws1.sinaimg.cn/large/006ZE1GEly1g3r7lh0i1sj30p00f1wfx.jpg" alt="公众号二维码"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;微信公众号：&lt;strong&gt;&lt;a href=&quot;#jump_20&quot;&gt;关注菜鸟解说大数据&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;关注可了解更多的大数据相关的内容。问题或建议，请公众号留言;&lt;br&gt;&lt;strong&gt;&lt;a href=&quot;#jump_20&quot;&gt;如果你觉
      
    
    </summary>
    
    
    
      <category term="CDH" scheme="https://junmoxiaopro.github.io/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH6.1.1中的hadoop的测试例子</title>
    <link href="https://junmoxiaopro.github.io/CDH6-1-1%E4%B8%AD%E7%9A%84hadoop%E7%9A%84%E6%B5%8B%E8%AF%95%E4%BE%8B%E5%AD%90.html"/>
    <id>https://junmoxiaopro.github.io/CDH6-1-1中的hadoop的测试例子.html</id>
    <published>2019-09-30T06:33:08.743Z</published>
    <updated>2019-09-30T06:33:22.332Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>微信公众号：<strong><a href="#jump_20">关注菜鸟解说大数据</a></strong><br>关注可了解更多的大数据相关的内容。问题或建议，请公众号留言;<br><strong><a href="#jump_20">如果你觉得我写的文章对你有帮助，欢迎关注和赞赏我</a>[^1]</strong></p></blockquote><p>[TOC]</p><h3 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h3><ul><li>1.<a href="https://mp.weixin.qq.com/s/GOdvWHxx11pHdURx9G9hcA" target="_blank" rel="noopener">在腾讯云中安装mysql</a></li><li>2.<a href="https://mp.weixin.qq.com/s/4QHc02NnmL8rVnY0q4tohg" target="_blank" rel="noopener">shell脚本出来Mysql的增删改查</a></li><li>3.<a href="https://mp.weixin.qq.com/s/DB-d3o-Zr6ma_5LlgUfcjQ" target="_blank" rel="noopener">MYSQL中limit不适用的场景</a></li><li>4.<a href="https://mp.weixin.qq.com/s/NPzgIx-UogYBTuut-Fe5ZA" target="_blank" rel="noopener">用shell玩转MYSQL实战</a></li></ul><h4 id="1-找到jar的目录"><a href="#1-找到jar的目录" class="headerlink" title="1.找到jar的目录"></a>1.找到jar的目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/jars</span><br></pre></td></tr></table></figure><h4 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2.如何使用"></a>2.如何使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.0.0-cdh6.1.1.jar  wordcount /tmp/data/wc.txt /tmp/data/out</span><br><span class="line">--说明：</span><br><span class="line">hadoop jar:执行jar的命令;</span><br><span class="line">hadoop-mapreduce-examples-3.0.0-cdh6.1.1.jar:执行的jar包的名字，这里因为我就在当前目录中有这个jar包所以我没有使用绝对路径，如果你不是在这个jar包所在的路径的话，需要使用绝对路径</span><br><span class="line">wordcount：程序主类的名字</span><br><span class="line">/tmp/data/wc.txt：文件的输入路径</span><br><span class="line">/tmp/data/out：文件的输出路径</span><br></pre></td></tr></table></figure><h4 id="3-运行过程"><a href="#3-运行过程" class="headerlink" title="3.运行过程"></a>3.运行过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">WARNING: Use &quot;yarn jar&quot; to launch YARN applications.</span><br><span class="line">19/09/26 16:25:18 INFO client.RMProxy: Connecting to ResourceManager at hadoop01/192.168.163.100:8032</span><br><span class="line">19/09/26 16:25:20 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /user/hdfs/.staging/job_1562050681525_0001</span><br><span class="line">19/09/26 16:25:21 INFO input.FileInputFormat: Total input files to process : 1</span><br><span class="line">19/09/26 16:25:22 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">19/09/26 16:25:22 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">19/09/26 16:25:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1562050681525_0001</span><br><span class="line">19/09/26 16:25:22 INFO mapreduce.JobSubmitter: Executing with tokens: []</span><br><span class="line">19/09/26 16:25:23 INFO conf.Configuration: resource-types.xml not found</span><br><span class="line">19/09/26 16:25:23 INFO resource.ResourceUtils: Unable to find &apos;resource-types.xml&apos;.</span><br><span class="line">19/09/26 16:25:25 INFO impl.YarnClientImpl: Submitted application application_1562050681525_0001</span><br><span class="line">19/09/26 16:25:26 INFO mapreduce.Job: The url to track the job: http://hadoop01:8088/proxy/application_1562050681525_0001/</span><br><span class="line">19/09/26 16:25:26 INFO mapreduce.Job: Running job: job_1562050681525_0001</span><br><span class="line">19/09/26 16:26:14 INFO mapreduce.Job: Job job_1562050681525_0001 running in uber mode : false</span><br><span class="line">19/09/26 16:26:14 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/09/26 16:27:14 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/09/26 16:27:23 INFO mapreduce.Job:  map 100% reduce 25%</span><br><span class="line">19/09/26 16:27:28 INFO mapreduce.Job:  map 100% reduce 50%</span><br><span class="line">19/09/26 16:27:34 INFO mapreduce.Job:  map 100% reduce 75%</span><br><span class="line">19/09/26 16:27:53 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/09/26 16:27:54 INFO mapreduce.Job: Job job_1562050681525_0001 completed successfully</span><br><span class="line">19/09/26 16:27:54 INFO mapreduce.Job: Counters: 54</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes read=191</span><br><span class="line">FILE: Number of bytes written=1088483</span><br><span class="line">FILE: Number of read operations=0</span><br><span class="line">FILE: Number of large read operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">HDFS: Number of bytes read=221</span><br><span class="line">HDFS: Number of bytes written=72</span><br><span class="line">HDFS: Number of read operations=23</span><br><span class="line">HDFS: Number of large read operations=0</span><br><span class="line">HDFS: Number of write operations=8</span><br><span class="line">HDFS: Number of bytes read erasure-coded=0</span><br><span class="line">Job Counters </span><br><span class="line">Launched map tasks=1</span><br><span class="line">Launched reduce tasks=4</span><br><span class="line">Data-local map tasks=1</span><br><span class="line">Total time spent by all maps in occupied slots (ms)=53647</span><br><span class="line">Total time spent by all reduces in occupied slots (ms)=42839</span><br><span class="line">Total time spent by all map tasks (ms)=53647</span><br><span class="line">Total time spent by all reduce tasks (ms)=42839</span><br><span class="line">Total vcore-milliseconds taken by all map tasks=53647</span><br><span class="line">Total vcore-milliseconds taken by all reduce tasks=42839</span><br><span class="line">Total megabyte-milliseconds taken by all map tasks=54934528</span><br><span class="line">Total megabyte-milliseconds taken by all reduce tasks=43867136</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=10</span><br><span class="line">Map output records=20</span><br><span class="line">Map output bytes=190</span><br><span class="line">Map output materialized bytes=175</span><br><span class="line">Input split bytes=101</span><br><span class="line">Combine input records=20</span><br><span class="line">Combine output records=10</span><br><span class="line">Reduce input groups=10</span><br><span class="line">Reduce shuffle bytes=175</span><br><span class="line">Reduce input records=10</span><br><span class="line">Reduce output records=10</span><br><span class="line">Spilled Records=20</span><br><span class="line">Shuffled Maps =4</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=4</span><br><span class="line">GC time elapsed (ms)=711</span><br><span class="line">CPU time spent (ms)=12920</span><br><span class="line">Physical memory (bytes) snapshot=1241501696</span><br><span class="line">Virtual memory (bytes) snapshot=12939522048</span><br><span class="line">Total committed heap usage (bytes)=904396800</span><br><span class="line">Peak Map Physical memory (bytes)=450433024</span><br><span class="line">Peak Map Virtual memory (bytes)=2578866176</span><br><span class="line">Peak Reduce Physical memory (bytes)=206004224</span><br><span class="line">Peak Reduce Virtual memory (bytes)=2591559680</span><br><span class="line">Shuffle Errors</span><br><span class="line">BAD_ID=0</span><br><span class="line">CONNECTION=0</span><br><span class="line">IO_ERROR=0</span><br><span class="line">WRONG_LENGTH=0</span><br><span class="line">WRONG_MAP=0</span><br><span class="line">WRONG_REDUCE=0</span><br><span class="line">File Input Format Counters </span><br><span class="line">Bytes Read=120</span><br><span class="line">File Output Format Counters </span><br><span class="line">Bytes Written=72</span><br><span class="line">您在 /var/spool/mail/root 中有新邮件</span><br></pre></td></tr></table></figure><h4 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h4><p><img src="http://ww1.sinaimg.cn/large/006ZE1GEly1g7d0dn3w23j311m04sq37.jpg" alt="55b156eedbe6fde87c3da2046a1b0be.png"></p><h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.examples;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.StringTokenizer;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line">public class WordCount &#123;</span><br><span class="line">    public WordCount() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();</span><br><span class="line">        if (otherArgs.length &lt; 2) &#123;</span><br><span class="line">            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);</span><br><span class="line">            System.exit(2);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf, &quot;word count&quot;);</span><br><span class="line">        job.setJarByClass(WordCount.class);</span><br><span class="line">        job.setMapperClass(WordCount.TokenizerMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCount.IntSumReducer.class);</span><br><span class="line">        job.setReducerClass(WordCount.IntSumReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        for(int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;</span><br><span class="line">            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));</span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">        private IntWritable result = new IntWritable();</span><br><span class="line"></span><br><span class="line">        public IntSumReducer() &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            int sum = 0;</span><br><span class="line"></span><br><span class="line">            IntWritable val;</span><br><span class="line">            for(Iterator var5 = values.iterator(); var5.hasNext(); sum += val.get()) &#123;</span><br><span class="line">                val = (IntWritable)var5.next();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            this.result.set(sum);</span><br><span class="line">            context.write(key, this.result);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">        private static final IntWritable one = new IntWritable(1);</span><br><span class="line">        private Text word = new Text();</span><br><span class="line"></span><br><span class="line">        public TokenizerMapper() &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            StringTokenizer itr = new StringTokenizer(value.toString());</span><br><span class="line"></span><br><span class="line">            while(itr.hasMoreTokens()) &#123;</span><br><span class="line">                this.word.set(itr.nextToken());</span><br><span class="line">                context.write(this.word, one);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a id="footnote-1"></a><br><a id="jump_20"></a></p><h3 id="关注菜鸟解说大数据"><a href="#关注菜鸟解说大数据" class="headerlink" title="关注菜鸟解说大数据"></a>关注菜鸟解说大数据</h3><p>如果你觉得到作者的文章对你有帮助，欢迎赞赏，有你的支持，公众号一定会越来越好！<br><img src="https://ws1.sinaimg.cn/large/006ZE1GEly1g3r7lh0i1sj30p00f1wfx.jpg" alt="公众号二维码"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;微信公众号：&lt;strong&gt;&lt;a href=&quot;#jump_20&quot;&gt;关注菜鸟解说大数据&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;关注可了解更多的大数据相关的内容。问题或建议，请公众号留言;&lt;br&gt;&lt;strong&gt;&lt;a href=&quot;#jump_20&quot;&gt;如果你觉
      
    
    </summary>
    
    
    
      <category term="hadoop" scheme="https://junmoxiaopro.github.io/tags/hadoop/"/>
    
  </entry>
  
</feed>
